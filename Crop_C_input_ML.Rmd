---
title: "Crop_C_input_ML"
author: "Dr. Chih-Yu Hung"
date: "2025-05-31"
output: html_document
---

## Macheine learnging method to estimate SOC change due to C input

```{r setup, include=FALSE}
library(tidyverse)
library(caret)
library(randomForest)
library(doParallel)
library(xgboost)
cl <- makeCluster(detectCores() - 1)
load(file="Input/SiteData_Daycent.RData")
```


## Random forest method

Random forest method with 10-fold training 

```{r Data preparation}

# Step 1: Prepare data (clean + factor handling)
SiteData <- SiteData %>%
  mutate(
    TEXCAT = as.factor(TEXCAT),
    TEXT = as.factor(TEXT)  # Make sure it's treated as a group variable
  ) %>%
  drop_na()

# Step 2: Split into three TEXT groups
group1_data <- SiteData %>% filter(TEXT == 1) #Coarse
group2_data <- SiteData %>% filter(TEXT == 2) #medium
group3_data <- SiteData %>% filter(TEXT == 3) #Fine

# Step 3: For each group, do 70/30 train-test split
set.seed(2025)
train1_idx <- createDataPartition(group1_data$deltaSOC, p = 0.7, list = FALSE)
train2_idx <- createDataPartition(group2_data$deltaSOC, p = 0.7, list = FALSE)
train3_idx <- createDataPartition(group3_data$deltaSOC, p = 0.7, list = FALSE)

trainData <- bind_rows(
  group1_data[train1_idx, ],
  group2_data[train2_idx, ],
  group3_data[train3_idx, ]
)

testData <- bind_rows(
  group1_data[-train1_idx, ],
  group2_data[-train2_idx, ],
  group3_data[-train3_idx, ]
)

```


```{r RF}
# Define tuning grid for Random Forest
tuneGrid <- expand.grid(.mtry = c(2, 4, 6, 8, 10))

# Set up explicit 10-fold cross-validation
control <- trainControl(method = "cv", number = 10)

# Train Random Forest model using k-fold cross-validation
set.seed(2025)
registerDoParallel(cl)
rf_model <- train(
  deltaSOC ~ . -Year -SiteName -Province -Ecodistrict -Area,
  #deltaSOC ~ . -Year -SiteName -Province -Ecodistrict -Area -totalHec -PERCECO -TCLAY -TSAND -TEXT,
  deltaSOC ~MAP,
  data = trainData,
  method = "rf",
  tuneGrid = tuneGrid,
  ntree = 500,
  trControl = control
)

stopCluster(cl)
# Check best tuning parameters
print(rf_model$bestTune)

# Predict and evaluate on independent test data
predictions <- predict(rf_model, newdata = testData)

# Calculate performance metrics
RMSE <- RMSE(predictions, testData$deltaSOC)
R2 <- R2(predictions, testData$deltaSOC)

# Output final performance
cat("Test Data RMSE:", RMSE, "\nTest Data R-squared:", R2, "\n")


```


##XGboost method


```{r XGBoost}
# Prepare training and testing data (assuming previous split)
# Define variables you want to include as predictors
selected_vars <- c("MAP", "MAT", "MPE", "TEXT", "CARB30", "Cinput") 

# Create training matrix
train_x <- trainData %>%
  select(all_of(selected_vars)) %>%
  mutate_if(is.factor, as.numeric) %>%
  as.matrix()

train_y <- trainData$deltaSOC

# Create testing matrix
test_x <- testData %>%
  select(all_of(selected_vars)) %>%
  mutate_if(is.factor, as.numeric) %>%
  as.matrix()

test_y <- testData$deltaSOC


# Define tuning grid
xgbGrid <- expand.grid(
  nrounds = c(200, 500, 800),       # number of boosting iterations
  max_depth = c(4, 6, 8),           # depth of trees
  eta = c(0.01, 0.05, 0.1),          # learning rate
  gamma = c(0,1),                        # minimum loss reduction
  colsample_bytree = c(0.6, 0.8),           # subsample ratio of columns
  min_child_weight = c(1,3),             # minimum instance weight
  subsample = c(0.6,0.8)                   # subsample ratio of the training instances
)

# Cross-validation control
control <- trainControl(method = "cv", number = 10)

set.seed(2025)
registerDoParallel(cl)
xgb_model <- train(
  x = train_x,
  y = train_y,
  #method = "xgbDART",
  method = "xgbTree",
  trControl = control,
  tuneGrid = xgbGrid
  #tuneLength = 10
)


stopCluster(cl)
# Best hyperparameters
print(xgb_model$bestTune)

predictions <- predict(xgb_model, newdata = test_x)

# Performance metrics
RMSE_xgb <- RMSE(predictions, test_y)
R2_xgb <- R2(predictions, test_y)

cat("XGBoost Test RMSE:", RMSE_xgb, "\nXGBoost Test R-squared:", R2_xgb)


```
